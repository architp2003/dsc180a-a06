{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f14357e5aa4a46aa39e50db999eb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:33<00:00,  2.96it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n",
      "/tmp/ipykernel_1694/1515279528.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9926451444625854\n",
      "Epoch 10, Loss: 1.2331691980361938\n",
      "Epoch 20, Loss: 0.8968768119812012\n",
      "Epoch 30, Loss: 0.7473369836807251\n",
      "Epoch 40, Loss: 0.6756051182746887\n",
      "Epoch 50, Loss: 0.6319088935852051\n",
      "Epoch 60, Loss: 0.6023977398872375\n",
      "Epoch 70, Loss: 0.5803974270820618\n",
      "Epoch 80, Loss: 0.5632628798484802\n",
      "Epoch 90, Loss: 0.549415647983551\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a49a1-e4ba-48e7-9bb0-67b3846204ae",
   "metadata": {},
   "source": [
    "**Answers To Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c0442-b822-475d-a5d0-46f4944d71d3",
   "metadata": {},
   "source": [
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4f22f-9cc0-434b-be86-8c9af53dedb9",
   "metadata": {},
   "source": [
    "Increasing the number of walks per node will result in more training time for learned embeddings. The learned embeddings will be able to make more generalizations of the model and be able to naviagte through complex scenarios. However, the learned embeddings may also be prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be84657-8c65-4dc9-acfd-f5fe5841508f",
   "metadata": {},
   "source": [
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bd4dd-8817-4680-af32-e639d5b1ba64",
   "metadata": {},
   "source": [
    "Decreasing the walk length could decrease the structural information captured by the embeddings. There could be a potential decrease in strongly connected components and some connections between nodes may not be identified at all, resulting in more local connections than global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220a2e4-4492-454d-b4a9-63142444e5e9",
   "metadata": {},
   "source": [
    "3. What would happen if we used directed edges instead of undirected edges for the random walks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56990f96-56b8-4495-a733-e22d91c6e521",
   "metadata": {},
   "source": [
    "Using directed edges would cause significant changes in paths as a restriction is imposed on movement within the nodes, which is possible with undirected edges. There could also be nodes which cannot reach other nodes due to the edge direction. There could also be cases where some paths are traversed more often than the others, and can result in some paths being considered more important than the others in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aae038-76a8-4156-a20c-feb5cf3b39d7",
   "metadata": {},
   "source": [
    "4. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5cabb-80b6-4357-9f53-24132612aa37",
   "metadata": {},
   "source": [
    "Giving a model more features will provide more information for the graph neural network to learn. This should lead in a lower loss in the model on the basis that useful information is present in the additonal features provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e5dec-1eec-453a-94b5-1932e32ce046",
   "metadata": {},
   "source": [
    "5. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86f9a3-263f-4f76-892b-3e112573bf38",
   "metadata": {},
   "source": [
    "The classifier performance would be determined based on the usefulness of the classes of the new dataset and how much data is present. More classes will result in a tougher classification task and could result in a decrease in the prediction accuracy of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2d86d-ed3f-4af6-a0bc-005bfac61c77",
   "metadata": {},
   "source": [
    "6. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1313061-2a6d-486a-b1a8-2b5845036c1c",
   "metadata": {},
   "source": [
    "Using a larger embedding dimension can increase the model performance by helping the model generalize to more complex scenarios and also train on more features. The training time would increase as the computational cost will rise and moee memoery will be required to store the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec239bc-069d-4d7c-a375-c2f3044923fe",
   "metadata": {},
   "source": [
    "**Answers to Extra Credit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f32d4-c3cc-43b8-8b63-fdde9d9d1da3",
   "metadata": {},
   "source": [
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00777977-252a-4a74-9968-45694b6aed56",
   "metadata": {},
   "source": [
    "Increasing the window size for the skip-gram model would result in more generalization of embeddings. Creating a larger window makes embeddings less specific and allows for a broader context instead of looking for specific patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
