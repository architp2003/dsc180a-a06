{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25cbbef-5225-4dfe-b01c-c01d06b293bc",
   "metadata": {},
   "source": [
    "# PyTorch & MNIST Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ffb13-d039-4023-b897-118a134a9fa0",
   "metadata": {},
   "source": [
    "Let's go through a simple example of Pytorch and MNIST as a way to level set NNs and the use of notebooks. This is our first introductory class, with the initial goal of level setting and kicking-off discussions on training neural networks.\n",
    "\n",
    "Please complete all challenges below for 10 (+ 2 extra) points in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3622662-4d3f-4b42-8f1c-d1b63177488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as pl\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094b84f-d36b-4936-9364-a6812076d007",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ff127-447e-4854-aa6c-e794e4875867",
   "metadata": {},
   "source": [
    "MNIST is probably the most traditionally used dataset for neural networks, as it is a relatively challenging problem in computer vision: recognizing single-digit numbers from a hand-written digital format. Classically, this dataset takes the form of samples of $28 \\times 28$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07bfef-c64c-4926-bf5c-d4955da85817",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469c45f-cc76-4308-b302-5288192a927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ])),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize(\n",
    "                                        (0.1307,), (0.3081,))\n",
    "                                ])),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c70ae-311c-41fe-9780-9cb91f98a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a311a-0090-4780-8c76-5a0ebca43e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "for i in range(6):\n",
    "    pl.subplot(2,3,i+1)\n",
    "    pl.tight_layout()\n",
    "    pl.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    pl.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    pl.xticks([])\n",
    "    pl.yticks([])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47132e3b-113b-4a78-a004-d8c54cc0193d",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44918747-e0d5-465c-b2f2-8ec065cdb0a6",
   "metadata": {},
   "source": [
    "We need to specify the model through a Python class. Below we show how to create a Feedforward Neural Network model using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d389c91-bc58-4a60-a9b8-9a205d5981c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a230969-61fb-4d6e-8b99-8dc12a79fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        input_dim = 28*28\n",
    "        num_classes = 10\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return F.log_softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017bb8f-de8f-4133-99d7-3ab30021344e",
   "metadata": {},
   "source": [
    "You'll need to instantiate this class as well as an optimizer, which will apply an algorithm to find the internal parameters of that model, such as matrix weights and biases. As an example, we will use the Stochastic Gradient Descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebfc80-28cf-4cf5-bee5-e730d14a3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.1\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce470749-5291-40f8-9ff6-f42a161dab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FeedforwardNeuralNetModel()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb7f9c-2102-490b-9b2e-8114251ee04e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdbe6c-332f-480c-9d9f-f46ff253d965",
   "metadata": {},
   "source": [
    "Next, we will define the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6be983-1bf6-43fe-9172-481538887466",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c8817-e6ed-4af8-9487-4ec1e70ffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd61e8f-621c-4679-b96f-9b6175f3dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # clears gradients\n",
    "        output = network(data.reshape(-1, 28*28))\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item())\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), f'./results/model_iteration-{epoch}.pth')\n",
    "            torch.save(optimizer.state_dict(), f'./results/optimizer_iteration-{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887895af-acc2-47db-b39d-080ea333813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f182b23-fd7c-460d-a440-d38d7ec633c2",
   "metadata": {},
   "source": [
    "<br />\n",
    "Alongside trainig, we will also monitor the performance of the model on a set of samples not seen during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5f60d-264a-4cb7-91bf-b62013fa8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data.reshape(-1, 28*28))\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' \\\n",
    "          .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2472ee-40e3-4b54-a37f-d46cb570e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf0d6c-98c0-4e4a-acbf-786a99892815",
   "metadata": {},
   "source": [
    "## Training and evaluating for multiple epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651146e4-13fe-4052-8bd7-1bf4d633ae41",
   "metadata": {},
   "source": [
    "Let's train now for all desired epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9fa2a-1d5a-4b42-8631-2ff9afb1de97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2, n_epochs + 1): # starts from the second iteration\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbd7dc-7bcc-4079-91fb-6ed6e5548268",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb0cb8-9ff9-4d62-9035-ffbbedb9eb22",
   "metadata": {},
   "source": [
    "Finally, we can inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf16c5a-a610-4f69-96f7-42b37fc13521",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.plot(train_counter, train_losses, color=(0.2, 0.2, 1.0))\n",
    "pl.scatter(test_counter[:-1], test_losses, color=(1.0, 0.2, 0.2))\n",
    "\n",
    "pl.legend(['Train Loss', 'Test Loss'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42011104-4d54-4198-834e-20c4c76389cd",
   "metadata": {},
   "source": [
    "It's always important to inspect anecdotes to convince yourself the model is behind as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94401f54-1f96-4ab1-89a8-c181c7192e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = network(example_data.reshape(1000, 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3673745-19dd-464c-90b6-8a07c8adfd6e",
   "metadata": {},
   "source": [
    "**Challenge: (2pt)** Can you explain why we are using `torch.no_grad()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831edae-2b61-4d78-b30b-40b8c2fea223",
   "metadata": {},
   "source": [
    "**Answer:** Using `torch.no_grad()` prevents the calculation of gradients, which improves the performance of machine learning models by decreasing the computation work and memory needed.`torch.nograd()` is especially useful for feed forward neural networks, where model weights do not need to be updated, speeding up the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb030fa-d62b-419f-ba72-5c1b6ac71b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "for i in range(6):\n",
    "    pl.subplot(2,3,i+1)\n",
    "    pl.tight_layout()\n",
    "    pl.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    pl.title(\"Prediction: {}\".format(output.data.max(1, keepdim=True)[1][i].item()))\n",
    "    pl.xticks([])\n",
    "    pl.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133bd4a-b40d-48ef-9bf7-c1dbaa01df50",
   "metadata": {},
   "source": [
    "**Challenge: (1pt)** Re-do the plot above, but showcasing only miss-classifications (i.e. cases in which the model did wrong)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae46b1-e066-4866-91b9-3bb8c0b8c3fb",
   "metadata": {},
   "source": [
    "**Answer:** Challenge completed in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f96ac1-450d-4a81-8c55-711ad0fff2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrect predictions\n",
    "inc_example_data = []\n",
    "inc_outputs = []\n",
    "idx = 6\n",
    "while idx < len(example_data) and len(inc_example_data) < 6:\n",
    "    predicted = output.data.max(1, keepdim=True)[1][idx].item()\n",
    "    if (predicted != example_targets[idx]):\n",
    "        inc_example_data.append(example_data[idx])\n",
    "        inc_outputs.append(predicted)\n",
    "    idx += 1\n",
    "\n",
    "# Plot incorrect example data\n",
    "for i in range(6):\n",
    "    pl.subplot(2,3,i+1)\n",
    "    pl.tight_layout()\n",
    "    pl.imshow(inc_example_data[i][0], cmap='gray', interpolation='none')\n",
    "    pl.title(\"Prediction: {}\".format(inc_outputs[i]))\n",
    "    pl.xticks([])\n",
    "    pl.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ad18c-2610-4c79-b4da-7112e03304de",
   "metadata": {},
   "source": [
    "## Loading trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cacec6b-1e09-4c41-92cf-404a4b0a601c",
   "metadata": {},
   "source": [
    "Eventually, you will want to load the model you trained in the past for either running inference or continue the training procedure. The functions we developed above save artifacts contain all of the metadata and data about the model, assuming you have the right model class. Let's inspect those files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced5e87-eb77-4882-9780-57009de7cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7317a-647b-4737-9ea4-03a84ee0bc83",
   "metadata": {},
   "source": [
    "To load a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fdf17-564c-488a-8dfa-1800f051e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = FeedforwardNeuralNetModel()\n",
    "model_state_dict = torch.load(\"results/model_iteration-1.pth\")\n",
    "trained_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a08cf-70e3-45a7-a127-a6ddd50b5d07",
   "metadata": {},
   "source": [
    "Before proceeding, let's inspect `model_state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af704e-3d15-4e6c-8e60-ad64cf68c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a281f3e-3a71-42d3-b8c7-3848a0e651f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict['fc1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a59542-0642-4a1a-87f9-b2147d05c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs = pl.subplots(3,3, figsize=(8,8))\n",
    "\n",
    "c = 0\n",
    "for ax in axs:\n",
    "    for sax in ax:\n",
    "        sax.imshow(model_state_dict['fc1.weight'][c].reshape((28,28)), \n",
    "                   cmap = pl.get_cmap('Blues'))\n",
    "        c += 1\n",
    "        sax.axis('off')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe103d-7860-45f1-8302-ddd8155a9f61",
   "metadata": {},
   "source": [
    "Let's do the same for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642116a-6edb-4c0c-9e74-5f2088d65e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(trained_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer_state_dict = torch.load(\"results/optimizer_iteration-3.pth\")\n",
    "optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38237ef4-a4fd-4417-a787-9c67c1985917",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd44fe-de50-4aa7-bde7-f0c9e0a288f1",
   "metadata": {},
   "source": [
    "**Challenge (1pt):** Can you explain the data in this dictionary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78ff1b-8717-4925-894d-67a1426cd334",
   "metadata": {},
   "source": [
    "**Answer**: State `0` represents the images where portions are identified in blue and white. State `1` represents the bias terms associated with each image present in the dictionary. The `param_groups` describe the parameters used during model training, such as learning rate (`lr`), momentum, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090364-a89d-4092-a3b6-95db76b1bc14",
   "metadata": {},
   "source": [
    "## Final challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322747a3-a689-4596-a254-6ab112ad2117",
   "metadata": {},
   "source": [
    "* **(1 pt)** What happens if you use only 10% of the available training data? Plot the difference in performance of the network.\n",
    "* **(0.5pt)** What happens if you remove 80% of all samples with label 5. Do you see a difference in performance? Is this difference homogeneous?\n",
    "* **(0.5pt)** What happens if you change parameters like the learnign rate and momentum? Plot the difference.\n",
    "* **(2pt)** Can you add more layers to this neural network? Start with one additional layer (often called \"hidden layer\"). What changes can you observe in doing so?\n",
    "* **(2pt)** Can you add regularization to this model? Look for L1, L2, and drop-out regularizations. What changes do you observe?\n",
    "* **[stretch] (2pt)** Can you change this model and turn it into a convolutional neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750d5a4-69ba-437e-8b5b-5a1a54b76072",
   "metadata": {},
   "source": [
    "* **(1 pt)** What happens if you use only 10% of the available training data? Plot the difference in performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874de75b-aee1-42f9-8b93-f8c6554fa76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mod(epoch, trainer, train_losses, train_counter, network, optimizer):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainer):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data.reshape(-1, 28*28))\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainer.dataset),\n",
    "                100. * batch_idx / len(trainer), loss.item())\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(trainer.dataset)))\n",
    "\n",
    "def test_mod(test_losses, network):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data.reshape(-1, 28*28))\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' \\\n",
    "          .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d933c-53f0-4576-b01b-c03f66ed2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                                           transform=torchvision.transforms.Compose([\n",
    "                                               torchvision.transforms.ToTensor(),\n",
    "                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                           ]))\n",
    "train_10 = torch.utils.data.Subset(train_dataset, list(range(int(0.1 * len(train_dataset)))))\n",
    "train_loader_10 = torch.utils.data.DataLoader(train_10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "network_10 = FeedforwardNeuralNetModel()\n",
    "optimizer_10 = optim.SGD(network_10.parameters(), lr=learning_rate, momentum=momentum)\n",
    "n_epochs = 5\n",
    "\n",
    "train_losses_10 = []\n",
    "train_counter_10 = []\n",
    "test_losses_10 = []\n",
    "test_counter_10 = [i*len(train_loader_10.dataset) for i in range(n_epochs + 1)]\n",
    "    \n",
    "# Training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_mod(epoch, train_loader_10, train_losses_10, train_counter_10, network_10, optimizer_10)\n",
    "    test_mod(test_losses_10, network_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf666c-2d55-4c6b-af50-0eeddf4420cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=(1.0, 0.2, 0.2))\n",
    "pl.scatter(test_counter_10[:-1], test_losses_10, color='purple')\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss 10% Training'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Test Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e9d8d-ef3c-4322-8075-60cb41c311a0",
   "metadata": {},
   "source": [
    "**Answer:** Using 10% of the available training data results in similar training performance and a higher testing set loss. As seen in the graph above, test set losses were higher for each epoch when trianed on 10 percent training data in comparison to the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2dae9-7d31-46d3-a730-bfccddbd29b6",
   "metadata": {},
   "source": [
    "* **(0.5pt)** What happens if you remove 80% of all samples with label 5. Do you see a difference in performance? Is this difference homogeneous?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a8a17-c152-480d-95b4-a39838264977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "five_idx = [i for i in range(len(train_loader.dataset)) if train_loader.dataset[i][1] == 5]\n",
    "all_idx = [i for i in range(len(train_loader.dataset))]\n",
    "to_remove = five_idx[:int(len(five_idx) * 0.8)]\n",
    "\n",
    "train_20 = torch.utils.data.Subset(train_dataset, list(set(all_idx) - set(to_remove)))\n",
    "train_20_loader = torch.utils.data.DataLoader(train_20, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "n_epochs = 5\n",
    "train_losses_20 = []\n",
    "train_counter_20 = []\n",
    "test_losses_20 = []\n",
    "test_counter_20 = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "network_20 = FeedforwardNeuralNetModel()\n",
    "optimizer_20 = optim.SGD(network_20.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for i in range(1,n_epochs+1):\n",
    "    train_mod(i, train_20_loader, train_losses, train_counter, network_20, optimizer_20)\n",
    "    test_mod(test_losses_20, network_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981f780-78d6-4f4f-a92f-93cb3d494b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=(1.0, 0.2, 0.2))\n",
    "pl.scatter(test_counter_20[:-1], test_losses_20, color='purple')\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss 20% 5 labels'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Test Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7425eaf-bbe1-4e01-9d26-6f07ef29b9f9",
   "metadata": {},
   "source": [
    "Looking at the plot above, we can see that the test loss increase for each iteration when 80 percent of all training data labeled \"5\" is taken out of the training set. Furthermore, the trend of each training sets scatterplot seems to follow a general pattern, which gives us reason to believe that there could be a homogeneous difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbeee31-3d42-4279-bd94-f85d8ddc513e",
   "metadata": {},
   "source": [
    "* **(0.5pt)** What happens if you change parameters like the learnign rate and momentum? Plot the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e4895-4f03-4016-9a26-b4793f94f88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrs = [0.1, 0.01, 0.001]\n",
    "momentums = [1, 0.5, 0.001]\n",
    "colors = ['red', 'blue', 'green']\n",
    "fig, ax = pl.subplots()\n",
    "results = []\n",
    "for lr, momentum in zip(lrs, momentums):\n",
    "    network = FeedforwardNeuralNetModel()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=lr, momentum=momentum)\n",
    "    n_epochs = 5\n",
    "    \n",
    "    train_losses_temp = []\n",
    "    train_counter_temp = []\n",
    "    test_losses_temp = []\n",
    "    test_counter_temp = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "    \n",
    "    for j in range(1,n_epochs+1):\n",
    "        train_mod(j, train_loader, train_losses_temp, train_counter_temp, network, optimizer)\n",
    "        test_mod(test_losses_temp, network)\n",
    "        \n",
    "    results.append(test_losses_temp)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399162d-ebd4-446b-a90a-f4f739542493",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], results[0], color=colors[0])\n",
    "pl.scatter(test_counter[:-1], results[1], color=colors[1])\n",
    "pl.scatter(test_counter[:-1], results[2], color=colors[2])\n",
    "\n",
    "pl.legend([f\"lr={lrs[0]},momentum={mementum[0]}\", f\"lr={lrs[1]},momentum={mementum[1]}\", f\"lr={lrs[2]},momentum={mementum[2]}\"], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Test Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed8fb2-79d5-4bfc-a8de-cc9de297feb6",
   "metadata": {},
   "source": [
    "Increasing the momentum results in an increase of test loss, and a similar effect can be observed with the learning rate. As learning rate determines how high a step is taken during gradient descent, large learning rate values can result in failure to converge to the optimal value. Too high of a momentum value can cause failure in converegence as well as it determine how fast convergence occurs, and large values can result in inconsistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726b868-94ce-42cf-936a-4aee1512398c",
   "metadata": {},
   "source": [
    "* **(2pt)** Can you add more layers to this neural network? Start with one additional layer (often called \"hidden layer\"). What changes can you observe in doing so?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9dba68-bb36-46c8-b0cd-538892b92aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
    "        input_dim = 28 * 28\n",
    "        hidden_dim1 = 512  \n",
    "        hidden_dim2 = 256\n",
    "        out_dim = 10 \n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.out = nn.Linear(hidden_dim2, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.out(x)\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "\n",
    "network_hid = FeedforwardNeuralNetModel2()\n",
    "optimizer_hid = optim.SGD(network_hid.parameters(), lr=learning_rate, momentum=momentum)\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses_hid = []\n",
    "train_counter_hid = []\n",
    "test_losses_hid = []\n",
    "test_counter_hid = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "network = FeedforwardNeuralNetModel2()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum) \n",
    "\n",
    "for i in range(1,n_epochs+1):\n",
    "    train_mod(i, train_loader, train_losses_hid, train_counter_hid, network_hid, optimizer_hid)\n",
    "    test_mod(test_losses_hid, network_hid)\n",
    "    train_mod(i, train_loader, train_losses, train_counter, network, optimizer)\n",
    "    test_mod(test_losses, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c7b73-e713-41c6-b9f4-26a855b409a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=\"red\")\n",
    "pl.scatter(test_counter_hid[:-1], test_losses_hid, color=\"purple\")\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss Additional Layers'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3584db8-1182-4d61-a8d0-02f02b1d4422",
   "metadata": {},
   "source": [
    "**Answer:** Yes, I was able to add more layers to the neural network, consisting of one input layer, three hidden layers, and one output layer. Adding these additional layers initally resulted in a rise in test loss for the first training sample, and later continued to either beat or match the test loss of the original training model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb17672-7497-4492-ba92-32476e45d0fe",
   "metadata": {},
   "source": [
    "* **(2pt)** Can you add regularization to this model? Look for L1, L2, and drop-out regularizations. What changes do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971e440-827e-4311-bff3-7bedd462c39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def l1_regularization(model, l1_lambda):\n",
    "    l1_norm = sum(param.abs().sum() for param in model.parameters())\n",
    "    return l1_lambda * l1_norm\n",
    "\n",
    "network_l1 = FeedforwardNeuralNetModel2()\n",
    "optimizer_l1 = optim.SGD(network_l1.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "n_epochs = 10\n",
    "log_interval = 10\n",
    "l1_lambda = 0.001 \n",
    "\n",
    "train_losses_l1 = []\n",
    "train_counter_l1 = []\n",
    "test_losses_l1 = []\n",
    "test_counter_l1 = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train_l1(epoch):\n",
    "    network_l1.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network_l1(data.reshape(-1, 28*28))\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        l1_loss = l1_regularization(network_l1, l1_lambda)\n",
    "        total_loss = loss + l1_loss\n",
    "\n",
    "        total_loss.backward()  \n",
    "        optimizer_l1.step()  \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss.item())\n",
    "            )\n",
    "            train_losses_l1.append(total_loss.item())\n",
    "            train_counter_l1.append((batch_idx * len(data)) + ((epoch - 1) * len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "network = FeedforwardNeuralNetModel2()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "n_epochs = 10\n",
    "for i in range(1, n_epochs + 1):\n",
    "    train_l1(i)\n",
    "    test_mod(test_losses_l1, network_l1)\n",
    "    train_mod(i, train_loader, train_losses, train_counter, network, optimizer)\n",
    "    test_mod(test_losses, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c78e21-a154-4df7-9a98-5dc1541aa9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=\"red\")\n",
    "pl.scatter(test_counter_l1[:-1], test_losses_l1, color=\"purple\")\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss L1 Reg'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb762c-793a-4db5-8c98-d5997041b094",
   "metadata": {},
   "source": [
    "After implementing L1 regularization in the neural network designed in the previous question, we can observe that the test loss is higher for each sample in comparison to the original model. Further optimization is required (such as searching for the best learning rate and momentum) in order to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f77538-2bec-4ae3-a441-8fd198748a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_l2 = FeedforwardNeuralNetModel2()\n",
    "l2_lambda = 0.001\n",
    "\n",
    "optimizer_l2 = optim.SGD(network_l2.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_lambda)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses_l2 = []\n",
    "train_counter_l2 = []\n",
    "test_losses_l2 = []\n",
    "test_counter_l2 = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "network = FeedforwardNeuralNetModel2()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "for i in range(1, n_epochs + 1):\n",
    "    train_mod(i, train_loader, train_losses_l2, train_counter_l2, network_l2, optimizer_l2)\n",
    "    test_mod(test_losses_l2, network_l2)\n",
    "\n",
    "    train_mod(i, train_loader, train_losses, train_counter, network, optimizer)\n",
    "    test_mod(test_losses, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7351596-acf5-4858-a846-7bc7fd6035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=\"red\")\n",
    "pl.scatter(test_counter_l2[:-1], test_losses_l2, color=\"purple\")\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss L2 Reg'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd56c8f-49b2-4712-ac0d-7101912b791a",
   "metadata": {},
   "source": [
    "**Mention how L2 performed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79793245-426c-43ad-85ec-8a94df674a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel3(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(FeedforwardNeuralNetModel3, self).__init__()\n",
    "        input_dim = 28 * 28\n",
    "        hidden_dim1 = 512  \n",
    "        hidden_dim2 = 256\n",
    "        out_dim = 10 \n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.out = nn.Linear(hidden_dim2, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        out = self.out(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "network_drop = FeedforwardNeuralNetModel3(dropout_prob=0.5)\n",
    "optimizer_drop = optim.SGD(network_drop.parameters(), lr=learning_rate, momentum=momentum)\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses_drop = []\n",
    "train_counter_drop = []\n",
    "test_losses_drop = []\n",
    "test_counter_drop = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "network = FeedforwardNeuralNetModel3(dropout_prob=0.0)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for i in range(1, n_epochs + 1):\n",
    "    train_mod(i, train_loader, train_losses_drop, train_counter_drop, network_drop, optimizer_drop)\n",
    "    test_mod(test_losses_drop, network_drop)\n",
    "\n",
    "    train_mod(i, train_loader, train_losses, train_counter, network, optimizer)\n",
    "    test_mod(test_losses, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730d284-22af-49ed-9c59-be53d275dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=\"red\")\n",
    "pl.scatter(test_counter_drop[:-1], test_losses_drop, color=\"purple\")\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss Dropout Reg'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cebe25-4edb-4bee-8d04-3c77d3717904",
   "metadata": {},
   "source": [
    "**Mention how Dropout reg performed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aeb52-e111-4f3b-b598-29a7ad839cc4",
   "metadata": {},
   "source": [
    "* **[stretch] (2pt)** Can you change this model and turn it into a convolutional neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bff64-edb2-4c6e-9431-fa412e6848a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        num_classes = 100  \n",
    "        num_classes2 = 10\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1, stride=2)  \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2) \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)          \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, num_classes)\n",
    "        self.out = nn.Linear(num_classes, num_classes2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        x = self.conv1(x) \n",
    "        x = self.conv2(x) \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.out(x)\n",
    "        \n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afaaa8-3024-4f6c-a538-50dc06fb9f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = CNN()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "train_losses_cnn = []\n",
    "train_counter_cnn = []\n",
    "test_losses_cnn = []\n",
    "\n",
    "n_epochs = 10\n",
    "test_counter_cnn = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "for i in range(1, n_epochs + 1):\n",
    "    train_mod(i, train_loader, train_losses_cnn, train_counter_cnn)\n",
    "    test_mod(test_losses_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6738f5-e0b9-403c-8a1d-be2101b5ee92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = pl.figure()\n",
    "pl.scatter(test_counter[:-1], test_losses, color=\"red\")\n",
    "pl.scatter(test_counter_cnn[:-1], test_losses_cnn, color=\"purple\")\n",
    "\n",
    "pl.legend(['Test Loss Original', 'Test Loss CNN'], loc='upper right', frameon=False)\n",
    "pl.xlabel('Training Samples')\n",
    "pl.ylabel('Log Likelihood Loss')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e358a09-bb0c-4a53-ab30-03deeab5bc6e",
   "metadata": {},
   "source": [
    "We can see in the graph above that the loss of the CNN model is still worse than our original model, indicating that additional optimization is required to yield best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
