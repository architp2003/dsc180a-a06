{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9408648014068604\n",
      "Epoch 10, Loss: 0.5527430176734924\n",
      "Epoch 20, Loss: 0.09263462573289871\n",
      "Epoch 30, Loss: 0.020280536264181137\n",
      "Epoch 40, Loss: 0.007526488043367863\n",
      "Epoch 50, Loss: 0.004197495058178902\n",
      "Epoch 60, Loss: 0.002971513429656625\n",
      "Epoch 70, Loss: 0.0023829475976526737\n",
      "Epoch 80, Loss: 0.002031512325629592\n",
      "Epoch 90, Loss: 0.0017838183557614684\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991b7e2-9903-4c27-bdfa-e58fae30c596",
   "metadata": {},
   "source": [
    "## **Answers To Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88ff00-8757-4981-b9ed-ec0bb65ab794",
   "metadata": {},
   "source": [
    "1. If we added more GCN layers, the complexity of the model will increase and the model will be more prone to overfitting. This would mean that we will expect a considerable decrease in the training loss and this would result in the model becoming more prone to over-smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f637065-07ef-435f-8e5a-36685f426bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.947688102722168\n",
      "Epoch 10, Loss: 0.5603798031806946\n",
      "Epoch 20, Loss: 0.03717956691980362\n",
      "Epoch 30, Loss: 0.004519958049058914\n",
      "Epoch 40, Loss: 0.0011941419215872884\n",
      "Epoch 50, Loss: 0.0005936055094935\n",
      "Epoch 60, Loss: 0.0004178214294370264\n",
      "Epoch 70, Loss: 0.0003418547858018428\n",
      "Epoch 80, Loss: 0.0003000610158778727\n",
      "Epoch 90, Loss: 0.0002725078084040433\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 3-layer GCN\n",
    "class GCN3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN3, self).__init__()\n",
    "        int_dim = 20\n",
    "        self.conv1 = GCNConv(input_dim, int_dim)\n",
    "        self.conv2 = GCNConv(int_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN3(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4db40c-5133-4668-9dc9-0bcfd9f6f2da",
   "metadata": {},
   "source": [
    "2. Increasing the number of hidden dimensions will result in an increase in the model's capacity. Providing the GCN model will more features will allow the model to learn more patterns, however it will also become prone to overfitting. It will also result in the model having to do more calculations, increasing the computational and memory cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42bc93b-8d73-4013-9c7c-f30efca6afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9554576873779297\n",
      "Epoch 10, Loss: 0.08689606934785843\n",
      "Epoch 20, Loss: 0.003936638589948416\n",
      "Epoch 30, Loss: 0.0007678308174945414\n",
      "Epoch 40, Loss: 0.0003390878264326602\n",
      "Epoch 50, Loss: 0.00022610061569139361\n",
      "Epoch 60, Loss: 0.00018675034516490996\n",
      "Epoch 70, Loss: 0.00016744981985539198\n",
      "Epoch 80, Loss: 0.000155505578732118\n",
      "Epoch 90, Loss: 0.00014664324407931417\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42948206-0649-41d1-b4b3-fa84ed805f97",
   "metadata": {},
   "source": [
    "3. In the code below, the sigmoid activation function is implemented in between the GCN layers. Comparing the training loss in comparison to the model using the relu activation function, we can see that the model performance does change and the training losses increase for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c648d8-802a-4494-8b9c-e235157e7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.048659324645996\n",
      "Epoch 10, Loss: 1.404330849647522\n",
      "Epoch 20, Loss: 0.9214096069335938\n",
      "Epoch 30, Loss: 0.5692194700241089\n",
      "Epoch 40, Loss: 0.34371981024742126\n",
      "Epoch 50, Loss: 0.21529003977775574\n",
      "Epoch 60, Loss: 0.14460553228855133\n",
      "Epoch 70, Loss: 0.10453751683235168\n",
      "Epoch 80, Loss: 0.08032123744487762\n",
      "Epoch 90, Loss: 0.0645800456404686\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCNSig(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNSig, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCNSig(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb43e-9c57-4caa-a6d1-b77adb54a6be",
   "metadata": {},
   "source": [
    "4. If we trained on only 10% of the nodes and tested on the remaining 90%, we should expect a decrease in performance and a possiblity of underfitting the trained model. As the model will not have much data to work with, it will not be able to generalize results and be unable to navigate complex scenarios. This will result in an increase in the test loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac71150-8b1b-45e6-96f7-4037e3378532",
   "metadata": {},
   "source": [
    "5. Based on the result yielded by the code below, using a different optimizer such as RMSprop instead of Adam does not affect the convergence speed, as we can see a consistent decrease in the test loss, indicating that the optimal value is being approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d2e0599-ecf7-4c44-a385-06d2d0f6ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9566665887832642\n",
      "Epoch 10, Loss: 0.04365529119968414\n",
      "Epoch 20, Loss: 0.01677924394607544\n",
      "Epoch 30, Loss: 0.009819929488003254\n",
      "Epoch 40, Loss: 0.006707995664328337\n",
      "Epoch 50, Loss: 0.00497392937541008\n",
      "Epoch 60, Loss: 0.003880862146615982\n",
      "Epoch 70, Loss: 0.003134372178465128\n",
      "Epoch 80, Loss: 0.002595312427729368\n",
      "Epoch 90, Loss: 0.002189916791394353\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2344f-07b0-4764-b78d-fc94f0398ae1",
   "metadata": {},
   "source": [
    "Extra credit:\n",
    "\n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf1de3-ee41-4ead-b9ea-21e8a7a1542d",
   "metadata": {},
   "source": [
    "**Solve the first question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d84f9-472a-459c-9a62-17b364757a73",
   "metadata": {},
   "source": [
    "2. In the code below, when we remove the log-softmax function in the output layer, the loss function still seems to be working correctly as we can see the training loss decrease in each iteration. One difference is that the loss value decreases faster, indicating that the model is at risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d11cf708-2c36-4ab9-bc51-c9e1a35ca452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9403178691864014\n",
      "Epoch 10, Loss: 0.05999862030148506\n",
      "Epoch 20, Loss: 0.022603914141654968\n",
      "Epoch 30, Loss: 0.013168002478778362\n",
      "Epoch 40, Loss: 0.00894375704228878\n",
      "Epoch 50, Loss: 0.006597674917429686\n",
      "Epoch 60, Loss: 0.005127208773046732\n",
      "Epoch 70, Loss: 0.004125987645238638\n",
      "Epoch 80, Loss: 0.003404885996133089\n",
      "Epoch 90, Loss: 0.002864376874640584\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "        \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
